{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f1020c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 32000\n",
      "Tokenizer special tokens: ['<s>', '</s>', '<unk>']\n",
      "Tokenizer pad token: None\n"
     ]
    }
   ],
   "source": [
    "# Load the meta-llama/Llama-2-7b-hf tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "# Print the tokenizer's vocab size\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer.get_vocab())}\")\n",
    "\n",
    "# Print the tokenizer's special tokens\n",
    "print(f\"Tokenizer special tokens: {tokenizer.all_special_tokens}\")\n",
    "\n",
    "# Print the tokenizer's pad token\n",
    "print(f\"Tokenizer pad token: {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45ef7634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 'horse':\n",
      "Tokens: [1, 10435]\n",
      "Decoded: ['<s>', 'horse']\n",
      "\n",
      "\n",
      "wrap_quotes: '\"horse\"'\n",
      "Tokens: [1, 376, 2015, 344, 29908]\n",
      "Decoded: ['<s>', '\"', 'hor', 'se', '\"']\n",
      "\n",
      "wrap_unicode_quotes: '\"horse\"'\n",
      "Tokens: [1, 376, 2015, 344, 29908]\n",
      "Decoded: ['<s>', '\"', 'hor', 'se', '\"']\n",
      "\n",
      "wrap_parentheses: '(horse)'\n",
      "Tokens: [1, 313, 2015, 344, 29897]\n",
      "Decoded: ['<s>', '(', 'hor', 'se', ')']\n",
      "\n",
      "prefix_hair_space: ' horse'\n",
      "Tokens: [1, 29871, 30118, 2015, 344]\n",
      "Decoded: ['<s>', '', '\\u200a', 'hor', 'se']\n",
      "\n",
      "prefix_zw_space: '​horse'\n",
      "Tokens: [1, 29871, 30166, 2015, 344]\n",
      "Decoded: ['<s>', '', '\\u200b', 'hor', 'se']\n",
      "\n",
      "prefix_zwj_space: '‍horse'\n",
      "Tokens: [1, 29871, 30722, 2015, 344]\n",
      "Decoded: ['<s>', '', '\\u200d', 'hor', 'se']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pass in a word \"horse\", then tokenize and print the decoded tokens\n",
    "# Pass in the same word with interventions, and print the decoded tokens\n",
    "# Interventions include:\n",
    "# wrap_quotes, wrap_unicode_quotes, wrap_parentheses, prefix_hair_space, prefix_zw_space, prefix_zwj_space, prefix_zwj_space_zwj, prefix_zwj_space_zwj_zwj\n",
    "\n",
    "entity = \"horse\"\n",
    "\n",
    "# Define intervention functions\n",
    "def wrap_quotes(text):\n",
    "    return f'\"{text}\"'\n",
    "\n",
    "def wrap_unicode_quotes(text):\n",
    "    return f'\"{text}\"'  # Unicode left/right double quotation marks\n",
    "\n",
    "def wrap_parentheses(text):\n",
    "    return f'({text})'\n",
    "\n",
    "def prefix_hair_space(text):\n",
    "    return '\\u200A' + text  # Hair space (U+200A)\n",
    "\n",
    "def prefix_zw_space(text):\n",
    "    return '\\u200B' + text  # Zero-width space (U+200B)\n",
    "\n",
    "def prefix_zwj_space(text):\n",
    "    return '\\u200D' + text  # Zero-width joiner (U+200D)\n",
    "\n",
    "\n",
    "interventions = [\n",
    "    (\"wrap_quotes\", wrap_quotes),\n",
    "    (\"wrap_unicode_quotes\", wrap_unicode_quotes),\n",
    "    (\"wrap_parentheses\", wrap_parentheses),\n",
    "    (\"prefix_hair_space\", prefix_hair_space),\n",
    "    (\"prefix_zw_space\", prefix_zw_space),\n",
    "    (\"prefix_zwj_space\", prefix_zwj_space),\n",
    "]\n",
    "\n",
    "# Tokenize the original entity\n",
    "tokens = tokenizer.encode(entity)\n",
    "print(f\"Original '{entity}':\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "# Decode the  tokens by decoding them separately and printing them iteratively\n",
    "print(f\"Decoded: {[tokenizer.decode(token) for token in tokens]}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Apply interventions and tokenize\n",
    "for intervention_name, intervention_func in interventions:\n",
    "    modified_entity = intervention_func(entity)\n",
    "    tokens = tokenizer.encode(modified_entity)\n",
    "    decoded = tokenizer.decode(tokens)\n",
    "    print(f\"{intervention_name}: '{modified_entity}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Decoded: {[tokenizer.decode(token) for token in tokens]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1d9543e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eos_token_id: 2\n",
      "unk_token_id: 0\n",
      "pad_token_id: None\n",
      "bos_token_id: 1\n",
      "eos_token: </s>\n",
      "unk_token: <unk>\n",
      "pad_token: None\n",
      "bos_token: <s>\n",
      "all_special_tokens: ['<s>', '</s>', '<unk>']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LlamaTokenizerFast' object has no attribute 'all_special_tokens_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbos_token: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer\u001b[38;5;241m.\u001b[39mbos_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_special_tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer\u001b[38;5;241m.\u001b[39mall_special_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_special_tokens_ids: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer\u001b[38;5;241m.\u001b[39mall_special_tokens_ids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaTokenizerFast' object has no attribute 'all_special_tokens_ids'"
     ]
    }
   ],
   "source": [
    "print(f\"eos_token_id: {tokenizer.eos_token_id}\")\n",
    "print(f\"unk_token_id: {tokenizer.unk_token_id}\")\n",
    "print(f\"pad_token_id: {tokenizer.pad_token_id}\")\n",
    "print(f\"bos_token_id: {tokenizer.bos_token_id}\")\n",
    "print(f\"eos_token: {tokenizer.eos_token}\")\n",
    "print(f\"unk_token: {tokenizer.unk_token}\")\n",
    "print(f\"pad_token: {tokenizer.pad_token}\")\n",
    "print(f\"bos_token: {tokenizer.bos_token}\")\n",
    "print(f\"all_special_tokens: {tokenizer.all_special_tokens}\")\n",
    "print(f\"all_special_tokens_ids: {tokenizer.all_special_tokens_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "924255b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2267b122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 32100\n",
      "Tokenizer special tokens: ['</s>', '<unk>', '<pad>', '<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']\n",
      "Tokenizer pad token: <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localdisk/ssrivas9/miniconda3/envs/lacts/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the T5 base tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "# Print the tokenizer's vocab size\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer.get_vocab())}\")\n",
    "\n",
    "# Print the tokenizer's special tokens\n",
    "print(f\"Tokenizer special tokens: {tokenizer.all_special_tokens}\")\n",
    "\n",
    "# Print the tokenizer's pad token\n",
    "print(f\"Tokenizer pad token: {tokenizer.pad_token}\")\n",
    "# Load the other T5 tokenizers \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae74625f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lacts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
